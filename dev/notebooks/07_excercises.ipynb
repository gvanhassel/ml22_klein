{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, \"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import make_dataset\n",
    "from src.models import imagemodels\n",
    "from src.models import train_model\n",
    "import gin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='model.gin', imports=['gin.torch.external_configurables'], includes=[])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `gin-config` to easily keep track of our experiments, and to easily save the different things we did during our experiments.\n",
    "\n",
    "The `model.gin` file is a simple file that will try to load parameters for funcitons that are already imported. \n",
    "\n",
    "So, if you wouldnt have imported train_model, the ginfile would not be able to parse settings for train_model.trainloop and will give an error.\n",
    "\n",
    "We can print all the settings that are operational with `gin.operative_config_str()` once we have loaded the functions to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while `.get_MNIST()` has two parameters that need to be set (a batchsize and a datadir), we can now load the function without having to do that: gin has done it already for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = make_dataset.get_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import gin.torch.external_configurables\n",
      "\n",
      "# Parameters for imagemodels.CNN:\n",
      "# ==============================================================================\n",
      "imagemodels.CNN.filter1 = 12\n",
      "imagemodels.CNN.filter2 = 5\n",
      "imagemodels.CNN.kernel_size = 2\n",
      "imagemodels.CNN.num_classes = 13\n",
      "\n",
      "# Parameters for CNN2:\n",
      "# ==============================================================================\n",
      "CNN2.filter1 = 12\n",
      "CNN2.filter2 = 5\n",
      "CNN2.kernel_size = 2\n",
      "CNN2.num_classes = 13\n",
      "CNN2.unit1 = 2\n",
      "\n",
      "# Parameters for get_MNIST:\n",
      "# ==============================================================================\n",
      "get_MNIST.batch_size = 32\n",
      "get_MNIST.data_dir = '../../data/raw'\n",
      "\n",
      "# Parameters for NeuralNetwork:\n",
      "# ==============================================================================\n",
      "NeuralNetwork.num_classes = 10\n",
      "NeuralNetwork.units1 = 512\n",
      "NeuralNetwork.units2 = 32\n",
      "\n",
      "# Parameters for trainloop:\n",
      "# ==============================================================================\n",
      "trainloop.epochs = 10\n",
      "trainloop.learning_rate = 0.001\n",
      "trainloop.log_dir = '../../models/gtest/'\n",
      "trainloop.loss_fn = @CrossEntropyLoss()\n",
      "trainloop.optimizer = @Adam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gin.config_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big advantage is that we can save this config as a file; that way it is easy to track what you changed during your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from src.models import metrics\n",
    "optimizer = optim.Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 12:51:03.250 | INFO     | src.data.data_tools:dir_add_timestamp:114 - Logging to ../../models/gtest/20221204-1251\n",
      "100%|██████████| 1875/1875 [00:45<00:00, 41.27it/s]\n",
      "2022-12-04 12:51:50.819 | INFO     | src.models.train_model:trainloop:171 - Epoch 0 train 0.6404 test 0.4929 metric ['0.8196']\n",
      "100%|██████████| 1875/1875 [00:40<00:00, 46.80it/s]\n",
      "2022-12-04 12:52:32.615 | INFO     | src.models.train_model:trainloop:171 - Epoch 1 train 0.3942 test 0.3946 metric ['0.8610']\n",
      "100%|██████████| 2/2 [01:28<00:00, 44.48s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "gin.parse_config_file(\"model.gin\")\n",
    "\n",
    "units = [512, 256, 128]\n",
    "learning_rate = [0.01, 0.001, 0.0001]\n",
    "kernel_size = 3 #default: 3\n",
    "filter1 = 64 #default: 32\n",
    "filter2 = 32 #default: 32\n",
    "unit1 = 64 #default: 64\n",
    "unit2 = 32 #default: 32\n",
    "epoch = 2\n",
    "lr = 0.001\n",
    "num_classes = 10\n",
    "\n",
    "# NeuralNetwork.num_classes=10\n",
    "# NeuralNetwork.units1 = 512\n",
    "\n",
    "# trainloop.epochs = 10\n",
    "# trainloop.learning_rate = 1e-3\n",
    "# trainloop.optimizer = @Adam\n",
    "# trainloop.loss_fn = @CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# for unit in units:\n",
    "#     for lr in learning_rate:\n",
    "\n",
    "gin.bind_parameter(\"CNN.num_classes\", num_classes)\n",
    "gin.bind_parameter(\"CNN.kernel_size\", kernel_size)\n",
    "gin.bind_parameter(\"CNN.filter1\", filter1)\n",
    "gin.bind_parameter(\"CNN.filter2\", filter2)\n",
    "# gin.bind_parameter(\"CNN.unit1\", unit1)\n",
    "# gin.bind_parameter(\"CNN.unit2\", unit2)\n",
    "\n",
    "gin.bind_parameter(\"trainloop.epochs\", epoch)\n",
    "gin.bind_parameter(\"trainloop.learning_rate\", lr)\n",
    "\n",
    "\n",
    "model = imagemodels.CNN()\n",
    "\n",
    "model =  train_model.trainloop(\n",
    "    # epochs=10,\n",
    "    model=model,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    train_steps=len(train_dataloader),\n",
    "    eval_steps=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, and study the result with tensorboard. \n",
    "\n",
    "Locally, it is easy to do that with VS code itself. On the server, you have to take these steps:\n",
    "\n",
    "- in the terminal, navigate to ~/code/ML22 \n",
    "- activate the python environment for the shell with `poetry shell`. Note how the correct environment is being activated.\n",
    "- run `tensorboard --logdir=models` in the terminal\n",
    "- tensorboard will launch at `localhost:6006` and vscode will notify you that the port is forwarded\n",
    "- you can either press the `launch` button in VScode or open your local browser at `localhost:6006`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Experiment with things like:\n",
    "\n",
    "- changing the amount of units1 and units2 to values between 16 and 1024. Use factors of 2: 16, 32, 64, etc.\n",
    "- changing the batchsize to values between 4 and 128. Again, use factors of two.\n",
    "- all your experiments are saved in the `models` directory, with a timestamp. Inside you find a saved_config.gin file, that \n",
    "contains all the settings for that experiment. The `events` file is what tensorboard will show.\n",
    "- plot the result in a heatmap: units vs batchsize.\n",
    "- changing the learningrate to values between 1e-2 and 1e-5 \n",
    "- changing the optimizer from SGD to one of the other available algoritms at [torch](https://pytorch.org/docs/stable/optim.html) (scroll down for the algorithms)\n",
    "\n",
    "A note on train_steps: this is a setting that determines how often you get an update. \n",
    "Because our complete dataset is 938 (60000 / 64) batches long, you will need 938 trainstep to cover the complete 60.000 images.\n",
    "\n",
    "This can actually be a bit confusion, because every value below 938 changes the meaning of `epoch` slightly, because one epoch is no longer\n",
    "the full dataset, but simply `trainstep` batches. Setting trainsteps to 100 means you need to wait twice as long before you get feedback on the performance,\n",
    "as compared to trainsteps=50. You will also see that settings trainsteps to 100 improves the learning, but that is simply because the model has seen twice as \n",
    "much examples as compared to trainsteps=50.\n",
    "\n",
    "This implies that it is not usefull to compare trainsteps=50 and trainsteps=100, because setting it to 100 will always be better.\n",
    "Just pick an amount, and adjust your number of epochs accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deep-learning-ho7aY0_Y-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "826f7c35c7cb2374ed015b71f995b28d51afc038e74920eb490e51986fe41e8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
